use anyhow::Result;
use chrono::{DateTime, Datelike, Duration, NaiveDate, Utc};
use polars::prelude::*;
use rayon::prelude::*;
use std::collections::{BTreeMap, HashMap, HashSet};
use std::fs::{self, File};
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};
use indicatif::{ProgressBar, ProgressStyle};


#[derive(Debug, Clone)]
pub struct DataGap {
    start_date: NaiveDate,
    end_date: NaiveDate,
    missing_days: i64,
}

#[derive(Debug)]
pub struct ProcessingStats {
    total_files: usize,
    processed_files: usize,
    #[allow(dead_code)]
    failed_files: usize,
    total_rows: usize,
    years_covered: Vec<i32>,
    gaps: Vec<DataGap>,
    gaps_by_year: HashMap<i32, Vec<DataGap>>,
}

pub struct EnhancedAnnualProcessor {
    base_dir: PathBuf,
    output_dir: PathBuf,
    processing_stats: Arc<Mutex<HashMap<String, ProcessingStats>>>,
    selected_dataset: Option<String>,
    year_filter: Option<Vec<i32>>,
}

impl EnhancedAnnualProcessor {
    pub fn new(base_dir: PathBuf) -> Self {
        let output_dir = base_dir.join("rollup_files");
        Self {
            base_dir,
            output_dir,
            processing_stats: Arc::new(Mutex::new(HashMap::new())),
            selected_dataset: None,
            year_filter: None,
        }
    }

    pub fn with_dataset(mut self, dataset: String) -> Self {
        self.selected_dataset = Some(dataset);
        self
    }

    pub fn with_years(mut self, years: Vec<i32>) -> Self {
        self.year_filter = Some(years);
        self
    }

    fn should_process_year(&self, year: i32) -> bool {
        match &self.year_filter {
            Some(years) => years.contains(&year),
            None => true,
        }
    }

    pub fn process_all_data(&self) -> Result<()> {
        let start_time = std::time::Instant::now();

        println!("üöÄ Enhanced Annual Parquet Processor");
        println!("üìÅ Base directory: {}", self.base_dir.display());
        println!("üìÇ Output directory: {}", self.output_dir.display());
        if let Some(ref dataset) = self.selected_dataset {
            println!("üéØ Selected dataset: {}", dataset);
        }
        if let Some(ref years) = self.year_filter {
            println!("üìÖ Year filter: {:?}", years);
        }
        println!("{}", "=".repeat(80));
        
        // Create output directory
        fs::create_dir_all(&self.output_dir)?;
        
        // Process each data type - RT prices last since they're the most voluminous
        let mut processors = vec![
            ("DA_prices", "DAM_Settlement_Point_Prices", ProcessorType::DayAheadPrice),
            ("AS_prices", "DAM_Clearing_Prices_for_Capacity", ProcessorType::AncillaryService),
            ("DAM_Gen_Resources", "60-Day_DAM_Disclosure_Reports", ProcessorType::DAMGenResource),
            ("DAM_Load_Resources", "60-Day_DAM_Disclosure_Reports", ProcessorType::DAMLoadResource),
            ("DAM_Energy_Bid_Awards", "60-Day_DAM_Disclosure_Reports", ProcessorType::DAMEnergyBidAwards),
            ("SCED_Gen_Resources", "60-Day_SCED_Disclosure_Reports", ProcessorType::SCEDGenResource),
            ("SCED_Load_Resources", "60-Day_SCED_Disclosure_Reports", ProcessorType::SCEDLoadResource),
            ("COP_Snapshots", "60-Day_COP_Adjustment_Period_Snapshot", ProcessorType::COPSnapshot),
            ("RT_prices", "Settlement_Point_Prices_at_Resource_Nodes,_Hubs_and_Load_Zones", ProcessorType::RealTimePrice),
            ("RT5minutes", "LMPs_by_Resource_Nodes,_Load_Zones_and_Trading_Hubs", ProcessorType::RealTime5MinutePrice),
        ];
        
        // Filter to selected dataset if specified
        if let Some(ref dataset) = self.selected_dataset {
            processors.retain(|(name, _, _)| name == dataset);
            if processors.is_empty() {
                println!("‚ùå Unknown dataset: {}", dataset);
                println!("Available datasets:");
                println!("  - DA_prices (Day-Ahead Settlement Point Prices)");
                println!("  - AS_prices (Ancillary Services Clearing Prices)");
                println!("  - DAM_Gen_Resources (60-Day DAM Generation Resources)");
                println!("  - DAM_Load_Resources (60-Day DAM Load Resources)");
                println!("  - DAM_Energy_Bid_Awards (60-Day DAM Energy Bid Awards - BESS charging!)");
                println!("  - SCED_Gen_Resources (60-Day SCED Generation Resources)");
                println!("  - SCED_Load_Resources (60-Day SCED Load Resources)");
                println!("  - COP_Snapshots (60-Day COP Adjustment Period Snapshots)");
                println!("  - RT_prices (Real-Time Settlement Point Prices - 15 minute)");
                println!("  - RT5minutes (Real-Time LMP Prices - 5 minute)");
                return Ok(());
            }
        }
        
        for (output_name, source_dir, processor_type) in processors {
            println!("\nüìä Processing: {}", output_name);
            println!("{}", "-".repeat(60));
            
            let source_path = self.base_dir.join(source_dir);
            if !source_path.exists() {
                println!("‚ö†Ô∏è  Source directory not found: {}", source_path.display());
                continue;
            }
            
            let output_path = self.output_dir.join(output_name);
            fs::create_dir_all(&output_path)?;
            
            match processor_type {
                ProcessorType::RealTimePrice => self.process_rt_prices(&source_path, &output_path)?,
                ProcessorType::RealTime5MinutePrice => self.process_rt5minutes(&source_path, &output_path)?,
                ProcessorType::DayAheadPrice => self.process_da_prices(&source_path, &output_path)?,
                ProcessorType::AncillaryService => self.process_as_prices(&source_path, &output_path)?,
                ProcessorType::DAMGenResource => self.process_dam_gen_resources(&source_path, &output_path)?,
                ProcessorType::DAMLoadResource => self.process_dam_load_resources(&source_path, &output_path)?,
                ProcessorType::DAMEnergyBidAwards => self.process_dam_energy_bid_awards(&source_path, &output_path)?,
                ProcessorType::SCEDGenResource => self.process_sced_gen_resources(&source_path, &output_path)?,
                ProcessorType::SCEDLoadResource => self.process_sced_load_resources(&source_path, &output_path)?,
                ProcessorType::COPSnapshot => self.process_cop_snapshots(&source_path, &output_path)?,
            }
        }
        
        // Generate overall status report
        self.generate_status_report()?;
        
        let elapsed = start_time.elapsed();
        println!("\n‚úÖ Processing complete!");
        println!("‚è±Ô∏è  Total time: {:?}", elapsed);
        
        Ok(())
    }
    
    fn process_rt_prices(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        if !csv_dir.exists() {
            let csv_dir = source_dir; // Try without csv subdirectory
            if !csv_dir.exists() {
                println!("‚ö†Ô∏è  No CSV directory found");
                return Ok(());
            }
        }
        
        // Get all CSV files
        let pattern = csv_dir.join("*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();
        
        println!("  Found {} RT price files", files.len());
        
        // Group by year and process
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }
        
        for (year, year_files) in files_by_year {
            if !self.should_process_year(year) {
                println!("  Skipping year {} (not in filter)", year);
                continue;
            }
            println!("  Processing year {}: {} files", year, year_files.len());
            
            let pb = ProgressBar::new(year_files.len() as u64);
            pb.set_style(ProgressStyle::default_bar()
                .template("[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} {msg}")
                .unwrap());
            
            // Process in parallel batches and combine incrementally
            let batch_size = 1000;  // Larger batches for better parallelism
            let mut combined_batches = Vec::new();
            
            for (batch_idx, batch) in year_files.chunks(batch_size).enumerate() {
                let batch_dfs: Vec<DataFrame> = batch
                    .par_iter()
                    .filter_map(|file| {
                        pb.inc(1);
                        match self.read_rt_price_file(file) {
                            Ok(df) => Some(df),
                            Err(e) => {
                                eprintln!("    Error reading {}: {}", file.display(), e);
                                None
                            }
                        }
                    })
                    .collect();
                
                // Combine this batch's dataframes
                if !batch_dfs.is_empty() {
                    if batch_idx % 10 == 9 {
                        pb.set_message(format!("combining batch {}", batch_idx + 1));
                    }
                    match self.combine_dataframes(batch_dfs) {
                        Ok(batch_df) => combined_batches.push(batch_df),
                        Err(e) => eprintln!("    Error combining batch {}: {}", batch_idx, e),
                    }
                }
            }
            
            pb.finish_with_message("done");
            
            if !combined_batches.is_empty() {
                println!("    Performing final combination of {} batch results...", combined_batches.len());
                // Final combination of all batch results
                let combined_df = self.combine_dataframes(combined_batches)?;
                
                // Check for gaps
                let gaps = self.detect_gaps(&combined_df, "datetime")?;
                
                // Save to parquet
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;
                
                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());
                
                // Save gaps report
                self.save_gaps_report(output_dir, year, &gaps)?;
                
                // Update stats
                self.update_stats("RT_prices", year, year_files.len(), combined_df.height(), gaps);
            }
        }
        
        // Save schema
        self.save_schema(output_dir, "RT_prices", r#"{
  "description": "Real-time settlement point prices at 15-minute intervals",
  "columns": {
    "datetime": {"type": "Datetime", "description": "Timestamp of the price interval"},
    "DeliveryDate": {"type": "Date", "description": "Operating day"},
    "DeliveryHour": {"type": "Float64", "description": "Hour of the day (1-24)"},
    "DeliveryInterval": {"type": "Float64", "description": "15-minute interval (1-4)"},
    "SettlementPointName": {"type": "Utf8", "description": "Settlement point name"},
    "SettlementPointType": {"type": "Utf8", "description": "Type: RN, HB, LZ"},
    "SettlementPointPrice": {"type": "Float64", "description": "Price in $/MWh"},
    "DSTFlag": {"type": "Utf8", "description": "Daylight Saving Time flag"}
  }
}"#)?;
        
        Ok(())
    }
    
    fn read_rt_price_file(&self, file: &Path) -> Result<DataFrame> {
        // Read CSV with Float64 for price column, keep dates as strings
        let schema_overrides = Schema::from_iter([
            Field::new("SettlementPointPrice", DataType::Float64),
            Field::new("DeliveryDate", DataType::String),
        ]);
        
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_try_parse_dates(false)  // Don't auto-parse dates
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .finish()?;
        
        // Normalize the dataframe to handle schema evolution (DST flag added in 2011)
        let mut df = crate::schema_normalizer::normalize_rt_prices(df)?;
        
        // Parse dates and create proper datetime column
        let delivery_dates = df.column("DeliveryDate")?;
        let delivery_hours = df.column("DeliveryHour")?.cast(&DataType::Int32)?;
        let delivery_intervals = df.column("DeliveryInterval")?.cast(&DataType::Int32)?;
        
        let hours = delivery_hours.i32()?
            .apply(|v| if v.unwrap_or(0) == 24 { Some(0) } else { v.map(|x| x - 1) });
        
        let minutes = delivery_intervals.i32()?
            .apply(|i| i.map(|v| (v - 1) * 15));
        
        let mut datetimes = Vec::new();
        for i in 0..df.height() {
            if let Some(date_str) = delivery_dates.str()?.get(i) {
                if let Ok(date) = NaiveDate::parse_from_str(date_str, "%m/%d/%Y") {
                    let hour = hours.get(i).unwrap_or(0) as u32;
                    let minute = minutes.get(i).unwrap_or(0) as u32;
                    let mut datetime = date.and_hms_opt(hour, minute, 0).unwrap();
                    
                    // Handle hour 24
                    if delivery_hours.i32()?.get(i) == Some(24) {
                        datetime += Duration::days(1);
                    }
                    
                    datetimes.push(Some(datetime.and_utc().timestamp_millis()));
                } else {
                    datetimes.push(None);
                }
            } else {
                datetimes.push(None);
            }
        }
        
        let datetime_series = Series::new("datetime", datetimes);
        df.with_column(datetime_series)?;
        
        // Ensure price is Float64
        df.with_column(df.column("SettlementPointPrice")?.cast(&DataType::Float64)?)?;
        
        Ok(df)
    }

    fn process_rt5minutes(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        // Use csv2/csv subdirectory for the new data
        let csv_dir = source_dir.join("csv2").join("csv");
        if !csv_dir.exists() {
            println!("‚ö†Ô∏è  No csv2/csv directory found at {}", source_dir.display());
            return Ok(());
        }

        // Get all CSV files
        let pattern = csv_dir.join("*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();

        println!("  Found {} RT 5-minute LMP files", files.len());

        // Group by year and process
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }

        for (year, year_files) in files_by_year {
            println!("  Processing year {}: {} files", year, year_files.len());

            let pb = ProgressBar::new(year_files.len() as u64);
            pb.set_style(ProgressStyle::default_bar()
                .template("[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} {msg}")
                .unwrap());

            // Process in parallel batches and combine incrementally
            // Reduced batch size to avoid "too many open files" error on macOS
            let batch_size = 100;  // Conservative batch size for file descriptors
            let mut combined_batches = Vec::new();

            for (batch_idx, batch) in year_files.chunks(batch_size).enumerate() {
                let batch_dfs: Vec<DataFrame> = batch
                    .par_iter()
                    .filter_map(|file| {
                        pb.inc(1);
                        match self.read_rt5minute_file(file) {
                            Ok(df) => Some(df),
                            Err(e) => {
                                eprintln!("    Error reading {}: {}", file.display(), e);
                                None
                            }
                        }
                    })
                    .collect();

                // Combine this batch's dataframes
                if !batch_dfs.is_empty() {
                    if batch_idx % 10 == 9 {
                        pb.set_message(format!("combining batch {}", batch_idx + 1));
                    }
                    match self.combine_dataframes(batch_dfs) {
                        Ok(batch_df) => combined_batches.push(batch_df),
                        Err(e) => eprintln!("    Error combining batch {}: {}", batch_idx, e),
                    }
                }
            }

            pb.finish_with_message("done");

            if !combined_batches.is_empty() {
                println!("    Performing final combination of {} batch results...", combined_batches.len());
                // Final combination of all batch results
                let combined_df = self.combine_dataframes(combined_batches)?;

                // Check for gaps
                let gaps = self.detect_gaps(&combined_df, "datetime")?;

                // Save to parquet
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;

                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());

                // Save gaps report
                self.save_gaps_report(output_dir, year, &gaps)?;

                // Update stats
                self.update_stats("RT5minutes", year, year_files.len(), combined_df.height(), gaps);
            }
        }

        // Save schema
        self.save_schema(output_dir, "RT5minutes", r#"{
  "description": "Real-time 5-minute LMP prices by resource nodes, load zones and trading hubs",
  "columns": {
    "datetime": {"type": "Datetime", "description": "SCED timestamp (5-minute intervals)"},
    "SCEDTimestamp": {"type": "Utf8", "description": "Original SCED timestamp string"},
    "RepeatedHourFlag": {"type": "Utf8", "description": "Flag for repeated hour during DST"},
    "SettlementPoint": {"type": "Utf8", "description": "Settlement point name"},
    "LMP": {"type": "Float64", "description": "Locational Marginal Price in $/MWh"}
  }
}"#)?;

        Ok(())
    }

    fn read_rt5minute_file(&self, file: &Path) -> Result<DataFrame> {
        // Read CSV with Float64 for LMP column
        let schema_overrides = Schema::from_iter([
            Field::new("LMP", DataType::Float64),
            Field::new("SCEDTimestamp", DataType::String),
        ]);

        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_try_parse_dates(false)  // Don't auto-parse dates
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .finish()?;

        // Parse SCEDTimestamp to create datetime column
        // Format: "12/01/2010 14:40:32"
        let sced_timestamps = df.column("SCEDTimestamp")?;

        let mut datetimes = Vec::new();
        for i in 0..df.height() {
            if let Some(timestamp_str) = sced_timestamps.str()?.get(i) {
                // Try to parse MM/DD/YYYY HH:MM:SS format
                if let Ok(datetime) = chrono::NaiveDateTime::parse_from_str(timestamp_str, "%m/%d/%Y %H:%M:%S") {
                    datetimes.push(Some(datetime.and_utc().timestamp_millis()));
                } else {
                    datetimes.push(None);
                }
            } else {
                datetimes.push(None);
            }
        }

        let datetime_series = Series::new("datetime", datetimes);
        let mut df = df;
        df.with_column(datetime_series)?;

        // Ensure LMP is Float64
        df.with_column(df.column("LMP")?.cast(&DataType::Float64)?)?;

        Ok(df)
    }

    fn process_da_prices(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        let csv_dir = if csv_dir.exists() { csv_dir } else { source_dir.to_path_buf() };
        
        // Pattern: cdr.00012331.*.DAMSPNP4190.csv
        let pattern = csv_dir.join("*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .filter(|p| {
                let name = p.file_name().unwrap().to_str().unwrap();
                name.contains("DAMSPNP4190") || name.contains("00012331")
            })
            .collect::<Vec<_>>();
        
        println!("  Found {} DA price files", files.len());
        
        // Group by year and process
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }
        
        for (year, year_files) in files_by_year {
            if !self.should_process_year(year) {
                println!("  Skipping year {} (not in filter)", year);
                continue;
            }
            println!("  Processing year {}: {} files", year, year_files.len());
            
            let pb = ProgressBar::new(year_files.len() as u64);
            pb.set_style(ProgressStyle::default_bar()
                .template("[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} {msg}")
                .unwrap());
            
            let batch_size = 100;
            let mut all_dfs = Vec::new();
            
            for batch in year_files.chunks(batch_size) {
                let batch_dfs: Vec<DataFrame> = batch
                    .par_iter()
                    .filter_map(|file| {
                        pb.inc(1);
                        match self.read_da_price_file(file) {
                            Ok(df) => Some(df),
                            Err(e) => {
                                eprintln!("    Error reading {}: {}", file.display(), e);
                                None
                            }
                        }
                    })
                    .collect();
                
                all_dfs.extend(batch_dfs);
            }
            
            pb.finish_with_message("done");
            
            if !all_dfs.is_empty() {
                println!("    Debug: Combining {} dataframes", all_dfs.len());
                let total_rows_before: usize = all_dfs.iter().map(|df| df.height()).sum();
                println!("    Debug: Total rows before combining: {}", total_rows_before);
                
                let combined_df = self.combine_dataframes(all_dfs)?;
                println!("    Debug: Rows after combining: {}", combined_df.height());
                
                // Check unique dates in the combined dataframe
                if combined_df.get_column_names().contains(&"DeliveryDate") {
                    let delivery_dates = combined_df.column("DeliveryDate")?;
                    let unique_dates = delivery_dates.unique()?;
                    println!("    Debug: Unique DeliveryDate values: {}", unique_dates.len());
                }
                
                let gaps = self.detect_gaps(&combined_df, "datetime")?;
                
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;
                
                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());
                
                self.save_gaps_report(output_dir, year, &gaps)?;
                self.update_stats("DA_prices", year, year_files.len(), combined_df.height(), gaps);
            }
        }
        
        self.save_schema(output_dir, "DA_prices", r#"{
  "description": "Day-ahead market settlement point prices (hourly)",
  "columns": {
    "datetime": {"type": "Datetime", "description": "Timestamp of the price hour"},
    "DeliveryDate": {"type": "Date", "description": "Operating day"},
    "HourEnding": {"type": "Utf8", "description": "Hour ending time"},
    "SettlementPoint": {"type": "Utf8", "description": "Settlement point name"},
    "SettlementPointPrice": {"type": "Float64", "description": "Price in $/MWh"},
    "DSTFlag": {"type": "Utf8", "description": "Daylight Saving Time flag"}
  }
}"#)?;
        
        Ok(())
    }
    
    fn read_da_price_file(&self, file: &Path) -> Result<DataFrame> {
        // Read CSV with Float64 for price column and string for dates initially
        let schema_overrides = Schema::from_iter([
            Field::new("SettlementPointPrice", DataType::Float64),
            Field::new("DeliveryDate", DataType::String),  // Read as string first
            Field::new("HourEnding", DataType::String),     // Read as string first
        ]);
        
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_try_parse_dates(false)  // Don't auto-parse dates
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .finish()?;
        
        // Normalize the dataframe to handle schema evolution (DST flag added in 2011)
        let df = crate::schema_normalizer::normalize_dam_prices(df)?;
        
        // Process the dataframe - first collect then parse dates
        let mut df = df.lazy()
            .with_column(
                col("SettlementPointPrice").cast(DataType::Float64)  // Critical: Force Float64
            )
            .with_column(
                col("HourEnding").alias("hour")
            )
            .collect()?;
        
        // Now parse the dates from MM/DD/YYYY string to Date type
        let mut parsed_dates: Vec<Option<i32>> = Vec::new();
        let mut date_strings: Vec<Option<String>> = Vec::new();
        
        // Unix epoch date for calculating days since epoch
        let epoch = NaiveDate::from_ymd_opt(1970, 1, 1).unwrap();
        
        // Extract dates and process them
        {
            let delivery_dates = df.column("DeliveryDate")?;
            if let Ok(dates_str) = delivery_dates.str() {
                for i in 0..dates_str.len() {
                    if let Some(date_str) = dates_str.get(i) {
                        if let Ok(date) = NaiveDate::parse_from_str(date_str, "%m/%d/%Y") {
                            // Calculate days since Unix epoch (1970-01-01)
                            let days_since_epoch = (date - epoch).num_days() as i32;
                            parsed_dates.push(Some(days_since_epoch));
                            
                            // Format as ISO date with timezone indicator
                            // ERCOT operates in Central Time (CT)
                            let formatted = format!("{}T00:00:00-06:00", date.format("%Y-%m-%d"));
                            date_strings.push(Some(formatted));
                        } else {
                            parsed_dates.push(None);
                            date_strings.push(None);
                        }
                    } else {
                        parsed_dates.push(None);
                        date_strings.push(None);
                    }
                }
            }
        }
        
        // Create new Date column and replace the string one
        let date_series = Series::new("DeliveryDate", parsed_dates);
        df.with_column(date_series.cast(&DataType::Date)?)?;
        
        // Add string representation with timezone
        let date_str_series = Series::new("DeliveryDateStr", date_strings);
        df.with_column(date_str_series)?;
        
        // Add datetime column as unix timestamp (milliseconds since epoch)
        // This will be computed from DeliveryDate + HourEnding
        let df = df.lazy()
            .with_column(
                lit(NULL).cast(DataType::Int64).alias("datetime_ms")
            )
            .collect()?;
        
        Ok(df)
    }
    
    fn process_as_prices(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        let csv_dir = if csv_dir.exists() { csv_dir } else { source_dir.to_path_buf() };
        
        // Pattern: cdr.00012329.*.DAMCPCNP4188.csv
        let pattern = csv_dir.join("*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .filter(|p| {
                let name = p.file_name().unwrap().to_str().unwrap();
                name.contains("DAMCPCNP4188") || name.contains("00012329")
            })
            .collect::<Vec<_>>();
        
        println!("  Found {} AS price files", files.len());
        
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }
        
        for (year, year_files) in files_by_year {
            if !self.should_process_year(year) {
                println!("  Skipping year {} (not in filter)", year);
                continue;
            }
            println!("  Processing year {}: {} files", year, year_files.len());
            
            let pb = ProgressBar::new(year_files.len() as u64);
            pb.set_style(ProgressStyle::default_bar()
                .template("[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} {msg}")
                .unwrap());
            
            let batch_size = 100;
            let mut all_dfs = Vec::new();
            
            for batch in year_files.chunks(batch_size) {
                let batch_dfs: Vec<DataFrame> = batch
                    .par_iter()
                    .filter_map(|file| {
                        pb.inc(1);
                        match self.read_as_price_file(file) {
                            Ok(df) => Some(df),
                            Err(e) => {
                                eprintln!("    Error reading {}: {}", file.display(), e);
                                None
                            }
                        }
                    })
                    .collect();
                
                all_dfs.extend(batch_dfs);
            }
            
            pb.finish_with_message("done");
            
            if !all_dfs.is_empty() {
                let combined_df = self.combine_dataframes(all_dfs)?;
                let gaps = self.detect_gaps(&combined_df, "datetime")?;
                
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;
                
                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());
                
                self.save_gaps_report(output_dir, year, &gaps)?;
                self.update_stats("AS_prices", year, year_files.len(), combined_df.height(), gaps);
            }
        }
        
        self.save_schema(output_dir, "AS_prices", r#"{
  "description": "Day-ahead ancillary services clearing prices",
  "columns": {
    "datetime": {"type": "Datetime", "description": "Timestamp of the price hour"},
    "DeliveryDate": {"type": "Date", "description": "Operating day"},
    "HourEnding": {"type": "Utf8", "description": "Hour ending time"},
    "AncillaryType": {"type": "Utf8", "description": "Type: REGUP, REGDN, RRS, ECRS, NSPIN"},
    "MCPC": {"type": "Float64", "description": "Market Clearing Price for Capacity in $/MW"},
    "DSTFlag": {"type": "Utf8", "description": "Daylight Saving Time flag"}
  }
}"#)?;
        
        Ok(())
    }
    
    fn read_as_price_file(&self, file: &Path) -> Result<DataFrame> {
        // Read CSV with Float64 for price column and string for dates initially
        let schema_overrides = Schema::from_iter([
            Field::new("MCPC", DataType::Float64),
            Field::new("DeliveryDate", DataType::String),  // Read as string first
            Field::new("HourEnding", DataType::String),     // Read as string first
        ]);
        
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_try_parse_dates(false)  // Don't auto-parse dates
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .finish()?;
        
        // Normalize the dataframe to handle schema evolution (DST flag added in 2011)
        let df = crate::schema_normalizer::normalize_as_prices(df)?;
        
        // Process the dataframe - first collect then parse dates
        let mut df = df.lazy()
            .with_column(
                col("MCPC").cast(DataType::Float64)  // Critical: Force Float64
            )
            .with_column(
                col("HourEnding").alias("hour")
            )
            .collect()?;
        
        // Now parse the dates from MM/DD/YYYY string to Date type
        let mut parsed_dates: Vec<Option<i32>> = Vec::new();
        let mut date_strings: Vec<Option<String>> = Vec::new();
        
        // Unix epoch date for calculating days since epoch
        let epoch = NaiveDate::from_ymd_opt(1970, 1, 1).unwrap();
        
        // Extract dates and process them
        {
            let delivery_dates = df.column("DeliveryDate")?;
            if let Ok(dates_str) = delivery_dates.str() {
                for i in 0..dates_str.len() {
                    if let Some(date_str) = dates_str.get(i) {
                        if let Ok(date) = NaiveDate::parse_from_str(date_str, "%m/%d/%Y") {
                            // Calculate days since Unix epoch (1970-01-01)
                            let days_since_epoch = (date - epoch).num_days() as i32;
                            parsed_dates.push(Some(days_since_epoch));
                            
                            // Format as ISO date with timezone indicator
                            // ERCOT operates in Central Time (CT)
                            let formatted = format!("{}T00:00:00-06:00", date.format("%Y-%m-%d"));
                            date_strings.push(Some(formatted));
                        } else {
                            parsed_dates.push(None);
                            date_strings.push(None);
                        }
                    } else {
                        parsed_dates.push(None);
                        date_strings.push(None);
                    }
                }
            }
        }
        
        // Create new Date column and replace the string one
        let date_series = Series::new("DeliveryDate", parsed_dates);
        df.with_column(date_series.cast(&DataType::Date)?)?;
        
        // Add string representation with timezone
        let date_str_series = Series::new("DeliveryDateStr", date_strings);
        df.with_column(date_str_series)?;
        
        // Add datetime column as unix timestamp (milliseconds since epoch)
        let df = df.lazy()
            .with_column(
                lit(NULL).cast(DataType::Int64).alias("datetime_ms")
            )
            .collect()?;
        
        Ok(df)
    }
    
    fn process_dam_gen_resources(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        let csv_dir = if csv_dir.exists() { csv_dir } else { source_dir.to_path_buf() };
        
        // Pattern: 60d_DAM_Gen_Resource_Data-DD-MMM-YY.csv
        let pattern = csv_dir.join("60d_DAM_Gen_Resource_Data-*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();
        
        println!("  Found {} DAM Gen Resource files", files.len());
        
        // Group by year
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_60day_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }
        
        for (year, year_files) in files_by_year {
            if !self.should_process_year(year) {
                println!("  Skipping year {} (not in filter)", year);
                continue;
            }
            println!("  Processing year {}: {} files", year, year_files.len());
            
            let pb = ProgressBar::new(year_files.len() as u64);
            pb.set_style(ProgressStyle::default_bar()
                .template("[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} {msg}")
                .unwrap());
            
            let mut all_dfs = Vec::new();
            
            for file in &year_files {
                pb.inc(1);
                match self.read_dam_gen_file(file) {
                    Ok(df) => all_dfs.push(df),
                    Err(e) => eprintln!("    Error reading {}: {}", file.display(), e),
                }
            }
            
            pb.finish_with_message("done");
            
            if !all_dfs.is_empty() {
                let combined_df = self.combine_dataframes(all_dfs)?;
                let gaps = self.detect_gaps(&combined_df, "datetime")?;
                
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;
                
                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());
                
                self.save_gaps_report(output_dir, year, &gaps)?;
                self.update_stats("DAM_Gen_Resources", year, year_files.len(), combined_df.height(), gaps);
            }
        }
        
        self.save_schema(output_dir, "DAM_Gen_Resources", r#"{
  "description": "60-day DAM generation resource data including BESS",
  "columns": {
    "datetime": {"type": "Datetime", "description": "Timestamp"},
    "DeliveryDate": {"type": "Date", "description": "Operating day"},
    "HourEnding": {"type": "Utf8", "description": "Hour ending"},
    "ResourceName": {"type": "Utf8", "description": "Resource name"},
    "ResourceType": {"type": "Utf8", "description": "Type (PWRSTR for BESS)"},
    "AwardedQuantity": {"type": "Float64", "description": "DAM energy award MW"},
    "SettlementPointName": {"type": "Utf8", "description": "Settlement point"},
    "EnergySettlementPointPrice": {"type": "Float64", "description": "Price $/MWh"}
  }
}"#)?;
        
        Ok(())
    }
    
    fn read_dam_gen_file(&self, file: &Path) -> Result<DataFrame> {
        // Force critical numeric columns to be Float64
        let mut schema_overrides = Schema::new();
        schema_overrides.with_column("LSL".into(), DataType::Float64);
        schema_overrides.with_column("HSL".into(), DataType::Float64);
        schema_overrides.with_column("Awarded Quantity".into(), DataType::Float64);
        schema_overrides.with_column("Energy Settlement Point Price".into(), DataType::Float64);
        schema_overrides.with_column("RegUp Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RegDown Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RRS Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RRSPFR Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RRSFFR Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RRSUFR Awarded".into(), DataType::Float64);
        schema_overrides.with_column("ECRS Awarded".into(), DataType::Float64);
        schema_overrides.with_column("ECRSSD Awarded".into(), DataType::Float64);
        schema_overrides.with_column("NonSpin Awarded".into(), DataType::Float64);
        
        // Add MCPC columns - treat as Utf8 to handle mixed string/float values
        schema_overrides.with_column("ECRS MCPC".into(), DataType::String);
        schema_overrides.with_column("RegUp MCPC".into(), DataType::Float64);
        schema_overrides.with_column("RegDown MCPC".into(), DataType::Float64);
        schema_overrides.with_column("RRS MCPC".into(), DataType::Float64);
        schema_overrides.with_column("NonSpin MCPC".into(), DataType::Float64);
        
        // Keep Delivery Date as Utf8 to parse it properly
        schema_overrides.with_column("Delivery Date".into(), DataType::String);
        schema_overrides.with_column("Hour Ending".into(), DataType::String);
        
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .infer_schema(None)  // Don't infer - use our explicit schema
            .finish()?;
        
        // Get available columns
        let columns = df.get_column_names();
        let mut select_cols = vec![];
        
        // Add date columns first - these are critical
        if columns.contains(&"Delivery Date") {
            select_cols.push(col("Delivery Date").alias("DeliveryDate"));
        } else {
            select_cols.push(lit("").alias("DeliveryDate"));
        }
        
        if columns.contains(&"Hour Ending") {
            select_cols.push(col("Hour Ending").alias("HourEnding"));
        } else {
            select_cols.push(lit("1").alias("HourEnding"));
        }
        
        // Add columns with consistent schema - use default values for missing columns
        // This ensures all DataFrames have the same columns for vstack
        
        // Required fields
        if columns.contains(&"Resource Name") {
            select_cols.push(col("Resource Name").alias("ResourceName"));
        } else {
            select_cols.push(lit("").alias("ResourceName"));
        }
        
        if columns.contains(&"Resource Type") {
            select_cols.push(col("Resource Type").alias("ResourceType"));
        } else {
            select_cols.push(lit("").alias("ResourceType"));
        }
        
        if columns.contains(&"Settlement Point Name") {
            select_cols.push(col("Settlement Point Name").alias("SettlementPointName"));
        } else {
            select_cols.push(lit("").alias("SettlementPointName"));
        }
        
        if columns.contains(&"LSL") {
            select_cols.push(col("LSL").cast(DataType::Float64));
        } else {
            select_cols.push(lit(0.0).alias("LSL"));
        }
        
        if columns.contains(&"HSL") {
            select_cols.push(col("HSL").cast(DataType::Float64));
        } else {
            select_cols.push(lit(0.0).alias("HSL"));
        }
        
        if columns.contains(&"Awarded Quantity") {
            select_cols.push(col("Awarded Quantity").cast(DataType::Float64).alias("AwardedQuantity"));
        } else {
            select_cols.push(lit(0.0).alias("AwardedQuantity"));
        }
        
        if columns.contains(&"Energy Settlement Point Price") {
            select_cols.push(col("Energy Settlement Point Price").cast(DataType::Float64).alias("EnergySettlementPointPrice"));
        } else {
            select_cols.push(lit(0.0).alias("EnergySettlementPointPrice"));
        }
        
        // AS awards - normalize column names
        if columns.contains(&"RegUp Awarded") {
            select_cols.push(col("RegUp Awarded").cast(DataType::Float64).alias("RegUpAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("RegUpAwarded"));
        }
        
        if columns.contains(&"RegDown Awarded") {
            select_cols.push(col("RegDown Awarded").cast(DataType::Float64).alias("RegDownAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("RegDownAwarded"));
        }
        
        // Handle RRS schema evolution - old format had "RRS Awarded", new has separate columns
        if columns.contains(&"RRS Awarded") {
            // Old format - single RRS column, map to RRSAwarded
            select_cols.push(col("RRS Awarded").cast(DataType::Float64).alias("RRSAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("RRSAwarded"));
        }
        
        // New RRS columns (added ~2022)
        if columns.contains(&"RRSPFR Awarded") {
            select_cols.push(col("RRSPFR Awarded").cast(DataType::Float64).alias("RRSPFRAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("RRSPFRAwarded"));
        }
        
        if columns.contains(&"RRSFFR Awarded") {
            select_cols.push(col("RRSFFR Awarded").cast(DataType::Float64).alias("RRSFFRAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("RRSFFRAwarded"));
        }
        
        if columns.contains(&"RRSUFR Awarded") {
            select_cols.push(col("RRSUFR Awarded").cast(DataType::Float64).alias("RRSUFRAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("RRSUFRAwarded"));
        }
        
        // ECRS columns
        if columns.contains(&"ECRS Awarded") {
            select_cols.push(col("ECRS Awarded").cast(DataType::Float64).alias("ECRSAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("ECRSAwarded"));
        }
        
        if columns.contains(&"ECRSSD Awarded") {
            select_cols.push(col("ECRSSD Awarded").cast(DataType::Float64).alias("ECRSSDAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("ECRSSDAwarded"));
        }
        
        if columns.contains(&"NonSpin Awarded") {
            select_cols.push(col("NonSpin Awarded").cast(DataType::Float64).alias("NonSpinAwarded"));
        } else {
            select_cols.push(lit(0.0).alias("NonSpinAwarded"));
        }
        
        // Add QSE submitted bid curve points (up to 10 pairs for DAM)
        for i in 1..=10 {
            let mw_col = format!("QSE submitted Curve-MW{}", i);
            let price_col = format!("QSE submitted Curve-Price{}", i);
            
            if columns.contains(&mw_col.as_str()) {
                select_cols.push(col(&mw_col).cast(DataType::Float64));
            }
            if columns.contains(&price_col.as_str()) {
                select_cols.push(col(&price_col).cast(DataType::Float64));
            }
        }
        
        // Build dataframe with available columns
        let df = df.lazy()
            .select(select_cols)
            .with_column(
                // Parse HourEnding as integer (it's a string like "1", "2", etc.)
                col("HourEnding").cast(DataType::Int32).alias("hour")
            )
            .collect()?;
        
        // Now create a proper datetime from date string and hour
        // The DeliveryDate is in MM/DD/YYYY format
        let mut df = df.lazy()
            .with_column(
                // Cast DeliveryDate string to proper date
                col("DeliveryDate")
                    .cast(DataType::String)
                    .alias("DeliveryDate")
            )
            .collect()?;
        
        // Parse dates from MM/DD/YYYY format to Date32
        let mut parsed_dates = Vec::new();
        let mut date_strings = Vec::new();
        let epoch = NaiveDate::from_ymd_opt(1970, 1, 1).unwrap();
        
        // Extract dates and process them
        {
            let delivery_dates = df.column("DeliveryDate")?;
            if let Ok(dates_str) = delivery_dates.str() {
                for i in 0..dates_str.len() {
                    if let Some(date_str) = dates_str.get(i) {
                        if let Ok(date) = NaiveDate::parse_from_str(date_str, "%m/%d/%Y") {
                            // Calculate days since Unix epoch (1970-01-01)
                            let days_since_epoch = (date - epoch).num_days() as i32;
                            parsed_dates.push(Some(days_since_epoch));
                            
                            // Format as ISO date with timezone indicator
                            let formatted = format!("{}T00:00:00-06:00", date.format("%Y-%m-%d"));
                            date_strings.push(Some(formatted));
                        } else {
                            parsed_dates.push(None);
                            date_strings.push(None);
                        }
                    } else {
                        parsed_dates.push(None);
                        date_strings.push(None);
                    }
                }
            }
        }
        
        // Create new Date column and replace the string one
        let date_series = Series::new("DeliveryDate", parsed_dates);
        df.with_column(date_series.cast(&DataType::Date)?)?;
        
        // Add string representation with timezone
        let date_str_series = Series::new("DeliveryDateStr", date_strings);
        df.with_column(date_str_series)?;
        
        // Create datetime column by combining date and hour
        let df = df.lazy()
            .with_column(
                // Hour Ending 1 = 00:00-01:00, so we subtract 1 hour
                (col("DeliveryDate").cast(DataType::Datetime(TimeUnit::Milliseconds, None)) +
                 duration_hours(col("hour") - lit(1)))
                .alias("datetime")
            )
            .collect()?;
        
        Ok(df)
    }
    
    fn process_sced_gen_resources(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        let csv_dir = if csv_dir.exists() { csv_dir } else { source_dir.to_path_buf() };

        // Pattern: 60d_SCED_Gen_Resource_Data-DD-MMM-YY.csv
        let pattern = csv_dir.join("60d_SCED_Gen_Resource_Data-*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();

        println!("  Found {} SCED Gen Resource files", files.len());

        // These files are very large, process year by year
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_60day_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }

        for (year, year_files) in files_by_year {
            if !self.should_process_year(year) {
                println!("  Skipping year {} (not in filter)", year);
                continue;
            }
            println!("  Processing year {}: {} files", year, year_files.len());

            // STREAMING SOLUTION: Process large years in batches to avoid massive concat
            // Reduced batch size to 20 to keep memory under 40GB limit
            let batch_size = if year_files.len() > 100 { 20 } else { year_files.len() };

            if batch_size < year_files.len() {
                println!("    Using streaming batch processing: {} files per batch", batch_size);
            }

            // First pass: collect all unique columns across ALL files
            println!("    Scanning schema across all files...");
            let mut all_columns = std::collections::HashSet::new();
            for file in &year_files {
                match self.read_sced_gen_file(file) {
                    Ok(df) => {
                        for col in df.get_column_names() {
                            all_columns.insert(col.to_string());
                        }
                    }
                    Err(e) => {
                        eprintln!("    Warning: Could not read {} for schema: {}", file.display(), e);
                    }
                }
            }
            let mut all_columns: Vec<String> = all_columns.into_iter().collect();
            all_columns.sort();
            println!("    Normalizing to {} columns", all_columns.len());

            // Second pass: process files in batches and write to temp files to avoid memory overflow
            let temp_dir = output_dir.join(format!("temp_{}", year));
            fs::create_dir_all(&temp_dir)?;
            let mut temp_files = Vec::new();

            for (batch_idx, batch) in year_files.chunks(batch_size).enumerate() {
                println!("    Processing batch {}/{} ({} files)...",
                    batch_idx + 1,
                    (year_files.len() + batch_size - 1) / batch_size,
                    batch.len()
                );

                // Read batch files in parallel
                let batch_file_dfs: Vec<DataFrame> = batch.par_iter()
                    .filter_map(|file| {
                        match self.read_sced_gen_file(file) {
                            Ok(df) => Some(df),
                            Err(e) => {
                                eprintln!("    Error reading {}: {}", file.display(), e);
                                None
                            }
                        }
                    })
                    .collect();

                if !batch_file_dfs.is_empty() {
                    // Normalize batch to common schema
                    let normalized_batch = self.normalize_to_schema(batch_file_dfs, &all_columns)?;

                    // Combine files within this batch
                    let mut batch_combined = self.combine_dataframes(normalized_batch)?;
                    println!("      Batch {} combined: {} rows", batch_idx + 1, batch_combined.height());

                    // Write batch to temporary parquet file to free memory
                    let temp_file = temp_dir.join(format!("batch_{}.parquet", batch_idx));
                    let mut file = std::fs::File::create(&temp_file)?;
                    ParquetWriter::new(&mut file).finish(&mut batch_combined)?;
                    temp_files.push(temp_file);
                    println!("      Batch {} written to temp file (memory freed)", batch_idx + 1);
                }
            }

            if !temp_files.is_empty() {
                println!("    Reading and combining {} temp files into final parquet...", temp_files.len());

                // Read back temp files and combine (still memory efficient due to streaming)
                let batch_dfs: Vec<DataFrame> = temp_files.iter()
                    .filter_map(|temp_file| {
                        match LazyFrame::scan_parquet(temp_file, Default::default()) {
                            Ok(lf) => match lf.collect() {
                                Ok(df) => Some(df),
                                Err(e) => {
                                    eprintln!("    Error reading temp file: {}", e);
                                    None
                                }
                            },
                            Err(e) => {
                                eprintln!("    Error scanning temp file: {}", e);
                                None
                            }
                        }
                    })
                    .collect();

                let combined_df = if batch_dfs.len() == 1 {
                    batch_dfs.into_iter().next().unwrap()
                } else {
                    self.combine_dataframes(batch_dfs)?
                };

                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;

                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());

                // Clean up temp files
                let _ = fs::remove_dir_all(&temp_dir);

                // SCED files are 5-minute, gap detection would be complex
                self.update_stats("SCED_Gen_Resources", year, year_files.len(), combined_df.height(), vec![]);
            }
        }

        Ok(())
    }
    
    /// Normalize DataFrames to a pre-computed schema (more efficient for batch processing)
    fn normalize_to_schema(&self, dfs: Vec<DataFrame>, all_columns: &[String]) -> Result<Vec<DataFrame>> {
        if dfs.is_empty() {
            return Ok(vec![]);
        }

        // Normalize each DataFrame to have all columns
        let mut normalized = Vec::new();
        for df in dfs {
            let mut select_exprs = Vec::new();
            let existing_cols = df.get_column_names();

            for col_name in all_columns {
                if existing_cols.contains(&col_name.as_str()) {
                    // Column exists, use it
                    select_exprs.push(col(col_name));
                } else {
                    // Column doesn't exist, create with null values
                    // Determine appropriate default based on column name
                    if col_name.contains("AS_") || col_name.contains("Price") ||
                       col_name.contains("MW") || col_name.contains("Point") {
                        // Numeric column - use 0.0
                        select_exprs.push(lit(0.0f64).alias(col_name));
                    } else {
                        // String column - use empty string
                        select_exprs.push(lit("").alias(col_name));
                    }
                }
            }

            let normalized_df = df.lazy()
                .select(select_exprs)
                .collect()?;

            normalized.push(normalized_df);
        }

        Ok(normalized)
    }
    
    fn read_sced_gen_file(&self, file: &Path) -> Result<DataFrame> {
        // Force critical numeric columns to be Float64 to prevent i64 inference
        let mut schema_overrides = Schema::new();
        schema_overrides.with_column("LSL".into(), DataType::Float64);
        schema_overrides.with_column("HSL".into(), DataType::Float64);
        schema_overrides.with_column("Base Point".into(), DataType::Float64);
        schema_overrides.with_column("Telemetered Net Output".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service RRS".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service RRSPFR".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service RRSFFR".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service RRSUFR".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service Reg-Up".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service Reg-Down".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service REGUP".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service REGDN".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service Non-Spin".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service NSRS".into(), DataType::Float64);  // Non-Spinning Reserve Service
        schema_overrides.with_column("Ancillary Service NSPIN".into(), DataType::Float64);  // Another variant
        schema_overrides.with_column("Ancillary Service ECRS".into(), DataType::Float64);
        schema_overrides.with_column("Ancillary Service ECRSSD".into(), DataType::Float64);
        
        // Add SCED curve price columns - these are ALWAYS Float64!
        // SCED1 curve points - up to 50 points to be absolutely sure!
        for i in 1..=50 {
            schema_overrides.with_column(format!("SCED1 Curve-MW{}", i).into(), DataType::Float64);
            schema_overrides.with_column(format!("SCED1 Curve-Price{}", i).into(), DataType::Float64);
        }
        
        // SCED2 curve points - up to 50 points to be absolutely sure!
        for i in 1..=50 {
            schema_overrides.with_column(format!("SCED2 Curve-MW{}", i).into(), DataType::Float64);
            schema_overrides.with_column(format!("SCED2 Curve-Price{}", i).into(), DataType::Float64);
        }
        
        // Output Schedule columns
        schema_overrides.with_column("Output Schedule".into(), DataType::Float64);
        schema_overrides.with_column("Output Schedule 2".into(), DataType::Float64);
        
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .infer_schema(None)  // Don't infer - use our explicit schema
            .finish()?;
        
        // Get available columns
        let columns = df.get_column_names();
        
        // Build select list with required columns
        let mut select_cols = vec![
            col("SCED Time Stamp").alias("SCEDTimeStamp"),
            col("Resource Name").alias("ResourceName"),
            col("Resource Type").alias("ResourceType"),
            col("Base Point").cast(DataType::Float64).alias("BasePoint"),
            col("HSL").cast(DataType::Float64),
            col("LSL").cast(DataType::Float64),
        ];
        
        // Add optional columns if they exist
        // NOTE: "Telemetered Net Output " has a trailing space in CSV!
        if columns.contains(&"Telemetered Net Output ") {
            select_cols.push(col("Telemetered Net Output ").cast(DataType::Float64).alias("TelemeteredNetOutput"));
        } else if columns.contains(&"Telemetered Net Output") {
            select_cols.push(col("Telemetered Net Output").cast(DataType::Float64).alias("TelemeteredNetOutput"));
        }
        
        // Add AS columns if they exist (these were added over time)
        // RRS was split into three categories after 2022
        if columns.contains(&"Ancillary Service RRS") {
            select_cols.push(col("Ancillary Service RRS").cast(DataType::Float64).alias("AS_RRS"));
        }
        if columns.contains(&"Ancillary Service RRSPFR") {
            select_cols.push(col("Ancillary Service RRSPFR").cast(DataType::Float64).alias("AS_RRSPFR"));
        }
        if columns.contains(&"Ancillary Service RRSFFR") {
            select_cols.push(col("Ancillary Service RRSFFR").cast(DataType::Float64).alias("AS_RRSFFR"));
        }
        if columns.contains(&"Ancillary Service RRSUFR") {
            select_cols.push(col("Ancillary Service RRSUFR").cast(DataType::Float64).alias("AS_RRSUFR"));
        }
        if columns.contains(&"Ancillary Service Reg-Up") {
            select_cols.push(col("Ancillary Service Reg-Up").cast(DataType::Float64).alias("AS_RegUp"));
        }
        if columns.contains(&"Ancillary Service Reg-Down") {
            select_cols.push(col("Ancillary Service Reg-Down").cast(DataType::Float64).alias("AS_RegDown"));
        }
        if columns.contains(&"Ancillary Service Non-Spin") {
            select_cols.push(col("Ancillary Service Non-Spin").cast(DataType::Float64).alias("AS_NonSpin"));
        }
        if columns.contains(&"Ancillary Service NSRS") {
            select_cols.push(col("Ancillary Service NSRS").cast(DataType::Float64).alias("AS_NSRS"));
        }
        if columns.contains(&"Ancillary Service ECRS") {
            select_cols.push(col("Ancillary Service ECRS").cast(DataType::Float64).alias("AS_ECRS"));
        }
        if columns.contains(&"Ancillary Service ECRSSD") {
            select_cols.push(col("Ancillary Service ECRSSD").cast(DataType::Float64).alias("AS_ECRSSD"));
        }
        
        // Add SCED1 bid curve points (up to 35 pairs)
        for i in 1..=35 {
            let mw_col = format!("SCED1 Curve-MW{}", i);
            let price_col = format!("SCED1 Curve-Price{}", i);
            
            if columns.contains(&mw_col.as_str()) {
                select_cols.push(col(&mw_col).cast(DataType::Float64));
            }
            if columns.contains(&price_col.as_str()) {
                select_cols.push(col(&price_col).cast(DataType::Float64));
            }
        }
        
        // Add SCED2 bid curve points (up to 35 pairs)
        for i in 1..=35 {
            let mw_col = format!("SCED2 Curve-MW{}", i);
            let price_col = format!("SCED2 Curve-Price{}", i);
            
            if columns.contains(&mw_col.as_str()) {
                select_cols.push(col(&mw_col).cast(DataType::Float64));
            }
            if columns.contains(&price_col.as_str()) {
                select_cols.push(col(&price_col).cast(DataType::Float64));
            }
        }
        
        // Apply selection and add datetime column
        let df = df.lazy()
            .select(select_cols)
            .with_column(
                col("SCEDTimeStamp").cast(DataType::String).alias("datetime")
            )
            .collect()?;
        
        Ok(df)
    }
    
    fn process_sced_load_resources(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        let csv_dir = if csv_dir.exists() { csv_dir } else { source_dir.to_path_buf() };
        
        // Pattern: 60d_Load_Resource_Data_in_SCED-DD-MMM-YY.csv
        let pattern = csv_dir.join("60d_Load_Resource_Data_in_SCED-*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();
        
        println!("  Found {} SCED Load Resource files", files.len());
        
        // Group by year
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_60day_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }
        
        for (year, year_files) in files_by_year {
            if !self.should_process_year(year) {
                println!("  Skipping year {} (not in filter)", year);
                continue;
            }
            println!("  Processing year {}: {} files", year, year_files.len());
            
            let mut all_dfs = Vec::new();
            
            for file in &year_files {
                match self.read_sced_load_file(file) {
                    Ok(df) => all_dfs.push(df),
                    Err(e) => eprintln!("    Error reading {}: {}", file.display(), e),
                }
            }
            
            if !all_dfs.is_empty() {
                let combined_df = self.combine_dataframes(all_dfs)?;
                
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;
                
                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());
                
                self.update_stats("SCED_Load_Resources", year, year_files.len(), combined_df.height(), vec![]);
            }
        }
        
        Ok(())
    }
    
    fn read_sced_load_file(&self, file: &Path) -> Result<DataFrame> {
        // Force critical numeric columns to be Float64
        let mut schema_overrides = Schema::new();
        schema_overrides.with_column("Max Power Consumption".into(), DataType::Float64);
        schema_overrides.with_column("LDL".into(), DataType::Float64);
        schema_overrides.with_column("HDL".into(), DataType::Float64);
        schema_overrides.with_column("Base Point".into(), DataType::Float64);
        schema_overrides.with_column("Telemetered Load".into(), DataType::Float64);
        schema_overrides.with_column("AS LRS".into(), DataType::Float64);
        schema_overrides.with_column("HASL".into(), DataType::Float64);
        schema_overrides.with_column("LASL".into(), DataType::Float64);
        
        // Add bid curve columns
        for i in 1..=10 {
            schema_overrides.with_column(format!("SCED Bid to Buy Curve-MW{}", i).into(), DataType::Float64);
            schema_overrides.with_column(format!("SCED Bid to Buy Curve-Price{}", i).into(), DataType::Float64);
        }
        
        // Define null values - empty strings should be treated as null for numeric columns
        let null_values = NullValues::AllColumns(vec!["".to_string(), "NA".to_string(), "N/A".to_string()]);
        
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .infer_schema(None)  // Don't infer - we have explicit schema
            .with_null_values(Some(null_values))  // Treat empty strings as null
            .finish()?;
        
        // Get available columns
        let columns = df.get_column_names();
        
        // Build select list - include ALL columns that exist
        let mut select_cols = vec![];
        
        // Always include key columns if they exist
        if columns.contains(&"SCED Time Stamp") {
            select_cols.push(col("SCED Time Stamp").alias("SCEDTimeStamp"));
        }
        // FIX: CSV has "Resource Name", not "Load Resource Name"
        if columns.contains(&"Resource Name") {
            select_cols.push(col("Resource Name").alias("ResourceName"));
        }
        if columns.contains(&"Settlement Point Name") {
            select_cols.push(col("Settlement Point Name").alias("SettlementPointName"));
        }
        // Add QSE and other metadata
        if columns.contains(&"QSE") {
            select_cols.push(col("QSE"));
        }
        if columns.contains(&"Telemetered Resource Status") {
            select_cols.push(col("Telemetered Resource Status").alias("TelemeteredStatus"));
        }
        if columns.contains(&"Low Power Consumption") {
            select_cols.push(col("Low Power Consumption").cast(DataType::Float64).alias("LowPowerConsumption"));
        }

        // Add numeric columns
        if columns.contains(&"Max Power Consumption") {
            select_cols.push(col("Max Power Consumption").cast(DataType::Float64).alias("MaxPowerConsumption"));
        }
        if columns.contains(&"LDL") {
            select_cols.push(col("LDL").cast(DataType::Float64));
        }
        if columns.contains(&"HDL") {
            select_cols.push(col("HDL").cast(DataType::Float64));
        }
        if columns.contains(&"Base Point") {
            select_cols.push(col("Base Point").cast(DataType::Float64).alias("BasePoint"));
        }
        if columns.contains(&"Telemetered Load") {
            select_cols.push(col("Telemetered Load").cast(DataType::Float64).alias("TelemeteredLoad"));
        }
        if columns.contains(&"AS LRS") {
            select_cols.push(col("AS LRS").cast(DataType::Float64).alias("AS_LRS"));
        }
        if columns.contains(&"HASL") {
            select_cols.push(col("HASL").cast(DataType::Float64));
        }
        if columns.contains(&"LASL") {
            select_cols.push(col("LASL").cast(DataType::Float64));
        }
        
        // Add SCED Bid to Buy curve points (up to 10 pairs)
        for i in 1..=10 {
            let mw_col = format!("SCED Bid to Buy Curve-MW{}", i);
            let price_col = format!("SCED Bid to Buy Curve-Price{}", i);
            
            if columns.contains(&mw_col.as_str()) {
                select_cols.push(col(&mw_col).cast(DataType::Float64));
            }
            if columns.contains(&price_col.as_str()) {
                select_cols.push(col(&price_col).cast(DataType::Float64));
            }
        }
        
        // Apply selection and add datetime column
        let df = df.lazy()
            .select(select_cols)
            .with_column(
                col("SCEDTimeStamp").cast(DataType::String).alias("datetime")
            )
            .collect()?;
        
        Ok(df)
    }
    
    fn process_dam_load_resources(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        let csv_dir = if csv_dir.exists() { csv_dir } else { source_dir.to_path_buf() };
        
        // Pattern: 60d_DAM_Load_Resource_Data-DD-MMM-YY.csv
        let pattern = csv_dir.join("60d_DAM_Load_Resource_Data-*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();
        
        println!("  Found {} DAM Load Resource files", files.len());
        
        // Group by year
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_60day_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }
        
        for (year, year_files) in files_by_year {
            if !self.should_process_year(year) {
                println!("  Skipping year {} (not in filter)", year);
                continue;
            }
            println!("  Processing year {}: {} files", year, year_files.len());
            
            let pb = ProgressBar::new(year_files.len() as u64);
            pb.set_style(ProgressStyle::default_bar()
                .template("[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} {msg}")
                .unwrap());
            
            let mut all_dfs = Vec::new();
            
            for file in &year_files {
                pb.inc(1);
                match self.read_dam_load_file(file) {
                    Ok(df) => all_dfs.push(df),
                    Err(e) => eprintln!("    Error reading {}: {}", file.display(), e),
                }
            }
            
            pb.finish_with_message("done");
            
            if !all_dfs.is_empty() {
                // Normalize all DataFrames to have the same columns before combining
                let normalized_dfs = self.normalize_dam_load_dataframes(all_dfs)?;
                let combined_df = self.combine_dataframes(normalized_dfs)?;
                let gaps = self.detect_gaps(&combined_df, "datetime")?;
                
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;
                
                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());
                
                self.save_gaps_report(output_dir, year, &gaps)?;
                self.update_stats("DAM_Load_Resources", year, year_files.len(), combined_df.height(), gaps);
            }
        }
        
        Ok(())
    }
    
    fn process_dam_energy_bid_awards(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        let csv_dir = if csv_dir.exists() { csv_dir } else { source_dir.to_path_buf() };
        
        // Pattern: 60d_DAM_EnergyBidAwards-DD-MMM-YY.csv
        let pattern = csv_dir.join("60d_DAM_EnergyBidAwards-*.csv");
        let files = glob::glob(pattern.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();
        
        println!("  Found {} DAM Energy Bid Award files", files.len());
        
        // Group by year
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_60day_filename(&file) {
                files_by_year.entry(year).or_insert_with(Vec::new).push(file);
            }
        }
        
        // Process each year
        for (year, year_files) in files_by_year {
            println!("  Processing {} ({} files)...", year, year_files.len());
            
            let mut all_dfs = Vec::new();
            for file in &year_files {
                match self.read_dam_energy_bid_awards_file(file) {
                    Ok(df) => all_dfs.push(df),
                    Err(e) => println!("    ‚ö†Ô∏è  Error reading {}: {}", file.display(), e),
                }
            }
            
            if !all_dfs.is_empty() {
                // Combine all dataframes
                let combined_df = if all_dfs.len() == 1 {
                    all_dfs.into_iter().next().unwrap()
                } else {
                    concat(
                        all_dfs.iter().map(|df| df.clone().lazy()).collect::<Vec<_>>().as_slice(),
                        UnionArgs::default(),
                    )?.collect()?
                };
                
                // Save as parquet
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = File::create(&output_file)?;
                ParquetWriter::new(&mut file)
                    .with_compression(ParquetCompression::Snappy)
                    .finish(&mut combined_df.clone())?;
                
                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());
                
                self.update_stats("DAM_Energy_Bid_Awards", year, year_files.len(), combined_df.height(), vec![]);
            }
        }
        
        Ok(())
    }
    
    fn read_dam_energy_bid_awards_file(&self, file: &Path) -> Result<DataFrame> {
        // Force critical columns to be Float64
        let mut schema_overrides = Schema::new();
        
        // Key columns for BESS charging analysis
        schema_overrides.with_column("Energy Only Bid Award in MW".into(), DataType::Float64);
        schema_overrides.with_column("Settlement Point Price".into(), DataType::Float64);
        
        // Read CSV
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .infer_schema(None)
            .finish()?;
        
        // Parse dates and standardize columns
        let df = df.lazy()
            .with_column(
                // Parse Delivery Date - handle as string to date conversion
                col("Delivery Date")
                    .cast(DataType::String)
                    .alias("DeliveryDate")
            )
            .with_column(
                // Simple hour to datetime conversion
                col("Hour Ending").cast(DataType::Int32).alias("hour")
            )
            .with_column(
                // Rename for consistency
                col("Energy Only Bid Award in MW").alias("EnergyBidAwardMW")
            )
            .with_column(
                col("Settlement Point").alias("SettlementPoint")
            )
            .with_column(
                col("Settlement Point Price").alias("SettlementPointPrice")
            )
            .collect()?;
        
        Ok(df)
    }
    
    fn read_dam_load_file(&self, file: &Path) -> Result<DataFrame> {
        // Force critical numeric columns to be Float64
        let mut schema_overrides = Schema::new();
        
        // Load resource specific columns
        schema_overrides.with_column("Max Power Consumption for Load Resource".into(), DataType::Float64);
        schema_overrides.with_column("Low Power Consumption for Load Resource".into(), DataType::Float64);
        
        // Award columns - ALL must be Float64
        schema_overrides.with_column("Energy Bid Award".into(), DataType::Float64);
        schema_overrides.with_column("AS Physical Responsive Awards".into(), DataType::Float64);
        schema_overrides.with_column("RRS Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RRSPFR Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RRSFFR Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RRSUFR Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RegUp Awarded".into(), DataType::Float64);
        schema_overrides.with_column("RegDown Awarded".into(), DataType::Float64);
        schema_overrides.with_column("NonSpin Awarded".into(), DataType::Float64);
        schema_overrides.with_column("ECRS Awarded".into(), DataType::Float64);
        schema_overrides.with_column("ECRSSD Awarded".into(), DataType::Float64);
        schema_overrides.with_column("ECRSMD Awarded".into(), DataType::Float64);
        
        // MCPC columns - ALL must be Float64
        schema_overrides.with_column("RRS MCPC".into(), DataType::Float64);
        schema_overrides.with_column("RRSPFR MCPC".into(), DataType::Float64);
        schema_overrides.with_column("RRSFFR MCPC".into(), DataType::Float64);
        schema_overrides.with_column("RRSUFR MCPC".into(), DataType::Float64);
        schema_overrides.with_column("RegUp MCPC".into(), DataType::Float64);
        schema_overrides.with_column("RegDown MCPC".into(), DataType::Float64);
        schema_overrides.with_column("NonSpin MCPC".into(), DataType::Float64);
        schema_overrides.with_column("ECRS MCPC".into(), DataType::Float64);
        
        // Read CSV with explicit schema
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .infer_schema(None)  // Don't infer - use our explicit schema
            .finish()?;
        
        // Build dataframe with ALL available columns
        let df = df.lazy()
            .with_column(
                // Parse HourEnding as integer
                col("Hour Ending").cast(DataType::Int32).alias("hour")
            )
            .collect()?;
        
        // First rename Delivery Date to DeliveryDate
        let mut df = df.lazy()
            .with_column(
                col("Delivery Date").alias("DeliveryDate")
            )
            .collect()?;
        
        // Parse dates from MM/DD/YYYY format to Date32
        let mut parsed_dates = Vec::new();
        let mut date_strings = Vec::new();
        let epoch = NaiveDate::from_ymd_opt(1970, 1, 1).unwrap();
        
        // Extract dates and process them
        {
            let delivery_dates = df.column("DeliveryDate")?;
            if let Ok(dates_str) = delivery_dates.str() {
                for i in 0..dates_str.len() {
                    if let Some(date_str) = dates_str.get(i) {
                        if let Ok(date) = NaiveDate::parse_from_str(date_str, "%m/%d/%Y") {
                            // Calculate days since Unix epoch (1970-01-01)
                            let days_since_epoch = (date - epoch).num_days() as i32;
                            parsed_dates.push(Some(days_since_epoch));
                            
                            // Format as ISO date with timezone indicator
                            let formatted = format!("{}T00:00:00-06:00", date.format("%Y-%m-%d"));
                            date_strings.push(Some(formatted));
                        } else {
                            parsed_dates.push(None);
                            date_strings.push(None);
                        }
                    } else {
                        parsed_dates.push(None);
                        date_strings.push(None);
                    }
                }
            }
        }
        
        // Create new Date column and replace the string one
        let date_series = Series::new("DeliveryDate", parsed_dates);
        df.with_column(date_series.cast(&DataType::Date)?)?;
        
        // Add string representation with timezone
        let date_str_series = Series::new("DeliveryDateStr", date_strings);
        df.with_column(date_str_series)?;
        
        // Create datetime column by combining date and hour
        let df = df.lazy()
            .with_column(
                // Hour Ending 1 = 00:00-01:00, so we subtract 1 hour
                (col("DeliveryDate").cast(DataType::Datetime(TimeUnit::Milliseconds, None)) +
                 duration_hours(col("hour") - lit(1)))
                .alias("datetime")
            )
            .collect()?;
        
        Ok(df)
    }
    
    fn normalize_dam_load_dataframes(&self, dfs: Vec<DataFrame>) -> Result<Vec<DataFrame>> {
        if dfs.is_empty() {
            return Ok(vec![]);
        }
        
        // Find all unique columns across all DataFrames
        let mut all_columns = std::collections::HashSet::new();
        for df in &dfs {
            for col in df.get_column_names() {
                all_columns.insert(col.to_string());
            }
        }
        
        // Sort columns for consistent ordering
        let mut all_columns: Vec<String> = all_columns.into_iter().collect();
        all_columns.sort();
        
        println!("    Normalizing to {} columns", all_columns.len());
        
        // Normalize each DataFrame to have all columns
        let mut normalized = Vec::new();
        for df in dfs {
            let mut select_exprs = Vec::new();
            let existing_cols = df.get_column_names();
            
            for col_name in &all_columns {
                if existing_cols.contains(&col_name.as_str()) {
                    select_exprs.push(col(col_name));
                } else {
                    // Create with appropriate default
                    if col_name.contains("Awarded") || col_name.contains("MCPC") || 
                       col_name.contains("Power") || col_name.contains("Consumption") {
                        select_exprs.push(lit(0.0f64).alias(col_name));
                    } else if col_name == "hour" || col_name.contains("Hour") {
                        select_exprs.push(lit(0i32).alias(col_name));
                    } else if col_name == "datetime" || col_name == "DeliveryDate" {
                        // Use existing datetime/date column
                        if existing_cols.contains(&"datetime") {
                            select_exprs.push(col("datetime").alias(col_name));
                        } else if existing_cols.contains(&"DeliveryDate") {
                            select_exprs.push(col("DeliveryDate").alias(col_name));
                        } else {
                            select_exprs.push(lit(NULL).alias(col_name));
                        }
                    } else {
                        select_exprs.push(lit("").alias(col_name));
                    }
                }
            }
            
            let normalized_df = df.lazy()
                .select(select_exprs)
                .collect()?;
                
            normalized.push(normalized_df);
        }
        
        Ok(normalized)
    }
    
    fn normalize_cop_dataframe(&self, mut df: DataFrame) -> Result<DataFrame> {
        // Normalize COP dataframes to have consistent columns
        // Evolution of COP file formats:
        // - 13 columns: Before Dec 13, 2022 (with single RRS column)
        // - 15 columns: Dec 13-31, 2022 (RRS split into RRSPFR/RRSFFR/RRSUFR)
        // - 16 columns: 2023 (added ECRS)
        // - 19 columns: 2024+ (added Minimum/Maximum/Hour Beginning Planned SOC)
        
        let height = df.height();
        let columns = df.get_column_names();
        let has_rrspfr = columns.contains(&"RRSPFR");
        let has_rrs = columns.contains(&"RRS");
        let has_ecrs = columns.contains(&"ECRS");
        let has_min_soc = columns.contains(&"Minimum SOC");
        let has_max_soc = columns.contains(&"Maximum SOC");
        let has_planned_soc = columns.contains(&"Hour Beginning Planned SOC");
        
        // If it's the old format with single RRS column, we need to:
        // 1. Copy RRS values to RRSPFR (as a reasonable default)
        // 2. Create RRSFFR and RRSUFR with nulls
        // 3. Remove the original RRS column
        if !has_rrspfr && has_rrs {
            // Get RRS values to copy to RRSPFR
            let rrs_col = df.column("RRS")?;
            df.with_column(rrs_col.clone().with_name("RRSPFR"))?;
            df.with_column(Series::new("RRSFFR", vec![None::<f64>; height]))?;
            df.with_column(Series::new("RRSUFR", vec![None::<f64>; height]))?;
            
            // Remove the original RRS column
            let cols_to_keep: Vec<&str> = df.get_column_names()
                .into_iter()
                .filter(|&name| name != "RRS")
                .collect();
            df = df.select(cols_to_keep)?;
        }
        
        // Add ECRS column if missing (added in 2023)
        if !has_ecrs {
            df.with_column(Series::new("ECRS", vec![None::<f64>; height]))?;
        }
        
        // Add SOC columns if missing (added in 2024)
        if !has_min_soc {
            df.with_column(Series::new("Minimum SOC", vec![None::<f64>; height]))?;
        }
        if !has_max_soc {
            df.with_column(Series::new("Maximum SOC", vec![None::<f64>; height]))?;
        }
        if !has_planned_soc {
            df.with_column(Series::new("Hour Beginning Planned SOC", vec![None::<f64>; height]))?;
        }
        
        // Ensure consistent column ordering
        let standard_columns = vec![
            "Delivery Date", "QSE Name", "Resource Name", "Hour Ending", "Status",
            "High Sustained Limit", "Low Sustained Limit", "High Emergency Limit", "Low Emergency Limit",
            "Reg Up", "Reg Down", "RRSPFR", "RRSFFR", "RRSUFR", "NSPIN", "ECRS",
            "Minimum SOC", "Maximum SOC", "Hour Beginning Planned SOC"
        ];
        
        // Select columns in the standard order
        let available_cols: Vec<&str> = standard_columns.iter()
            .filter(|&&col| df.get_column_names().contains(&col))
            .copied()
            .collect();
            
        df = df.select(available_cols)?;
        
        Ok(df)
    }
    
    fn process_cop_snapshots(&self, source_dir: &Path, output_dir: &Path) -> Result<()> {
        let csv_dir = source_dir.join("csv");
        let csv_dir = if csv_dir.exists() { csv_dir } else { source_dir.to_path_buf() };
        
        // Pattern 1: 60d_COP_Adjustment_Period_Snapshot-DD-MMM-YY.csv (after Dec 13, 2022)
        let pattern1 = csv_dir.join("60d_COP_Adjustment_Period_Snapshot-*.csv");
        let files1 = glob::glob(pattern1.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();
        
        // Pattern 2: CompleteCOP_MMDDYYYY.csv (before Dec 13, 2022)
        let pattern2 = csv_dir.join("CompleteCOP_*.csv");
        let files2 = glob::glob(pattern2.to_str().unwrap())?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();
        
        let mut files = files1;
        files.extend(files2);
        
        println!("  Found {} COP Snapshot files total", files.len());
        
        let mut files_by_year: BTreeMap<i32, Vec<PathBuf>> = BTreeMap::new();
        for file in files {
            if let Some(year) = extract_year_from_60day_filename(&file) {
                files_by_year.entry(year).or_default().push(file);
            }
        }
        
        for (year, year_files) in files_by_year {
            if !self.should_process_year(year) {
                println!("  Skipping year {} (not in filter)", year);
                continue;
            }
            println!("  Processing year {}: {} files", year, year_files.len());
            
            let mut all_dfs = Vec::new();
            
            for file in &year_files {
                match self.read_cop_file(file) {
                    Ok(df) => {
                        // Normalize to handle schema evolution
                        match self.normalize_cop_dataframe(df) {
                            Ok(normalized) => all_dfs.push(normalized),
                            Err(e) => eprintln!("    Error normalizing {}: {}", file.display(), e),
                        }
                    },
                    Err(e) => eprintln!("    Error reading {}: {}", file.display(), e),
                }
            }
            
            if !all_dfs.is_empty() {
                let combined_df = self.combine_dataframes(all_dfs)?;
                
                let output_file = output_dir.join(format!("{}.parquet", year));
                let mut file = std::fs::File::create(&output_file)?;
                ParquetWriter::new(&mut file).finish(&mut combined_df.clone())?;
                
                println!("    ‚úÖ Saved {} rows to {}", combined_df.height(), output_file.display());
                
                self.update_stats("COP_Snapshots", year, year_files.len(), combined_df.height(), vec![]);
            }
        }
        
        Ok(())
    }
    
    fn read_cop_file(&self, file: &Path) -> Result<DataFrame> {
        // Use the robust COP file reader that handles all format variations
        // This handles late 2014 files without headers, schema evolution, etc.
        crate::cop_file_reader::read_cop_file(file)
    }
    
    fn _read_csv_with_schema_detection(&self, file: &Path) -> Result<DataFrame> {
        // Step 1: Read a sample to detect the actual columns
        let sample_df = CsvReader::from_path(file)?
            .has_header(true)
            .infer_schema(Some(2000))  // Sample enough rows to understand the data
            .finish()?;
        
        // Step 2: Build intelligent schema based on column name patterns
        let mut schema_overrides = Schema::new();
        
        // Define patterns for different data types
        let text_patterns = vec![
            "Date", "Hour", "Name", "Type", "QSE", "Settlement", "Status", 
            "Resource Name", "Load Resource", "Gen Resource", "Point"
        ];
        
        let numeric_patterns = vec![
            "Awarded", "MCPC", "Power", "Consumption", "MW", "Price", "Curve",
            "LSL", "HSL", "LDL", "HDL", "Base Point", "Output", "Telemetered",
            "Limit", "Schedule", "Bid", "Offer", "Quantity", "Capacity",
            "AS ", "RegUp", "RegDown", "RRS", "ECRS", "NonSpin", "NSRS"
        ];
        
        for col_name in sample_df.get_column_names() {
            let col_str = col_name.to_string();
            let col_lower = col_str.to_lowercase();
            
            // Check if it's definitely a text column
            let is_text = text_patterns.iter().any(|pattern| {
                col_lower.contains(&pattern.to_lowercase())
            }) && !numeric_patterns.iter().any(|pattern| {
                col_lower.contains(&pattern.to_lowercase())
            });
            
            // Check if it's definitely a numeric column  
            let is_numeric = numeric_patterns.iter().any(|pattern| {
                col_lower.contains(&pattern.to_lowercase())
            });
            
            let dtype = if is_text {
                DataType::String
            } else if is_numeric {
                DataType::Float64
            } else {
                // For unknown columns, check the actual data in the sample
                match sample_df.column(col_name) {
                    Ok(col) => {
                        // Check if the column contains numeric-looking data
                        match col.dtype() {
                            DataType::Int64 | DataType::Float64 | 
                            DataType::Int32 | DataType::Float32 => DataType::Float64,
                            _ => DataType::String,
                        }
                    },
                    Err(_) => DataType::String,  // Default to string if we can't determine
                }
            };
            
            schema_overrides.with_column(col_str.into(), dtype);
        }
        
        // Step 3: Define comprehensive null values
        let null_values = NullValues::AllColumns(vec![
            "".to_string(),
            "NA".to_string(), 
            "N/A".to_string(),
            "null".to_string(),
            "NULL".to_string(),
            "#N/A".to_string(),
            "NaN".to_string(),
        ]);
        
        // Step 4: Read the file with the determined schema
        let df = CsvReader::from_path(file)?
            .has_header(true)
            .with_dtypes(Some(Arc::new(schema_overrides)))
            .infer_schema(None)  // Don't infer - use our schema
            .with_null_values(Some(null_values))
            .finish()?;
        
        Ok(df)
    }
    
    fn combine_dataframes(&self, dfs: Vec<DataFrame>) -> Result<DataFrame> {
        if dfs.is_empty() {
            return Err(anyhow::anyhow!("No dataframes to combine"));
        }

        // Normalize schemas - collect all unique columns with their types
        let mut column_types: HashMap<String, DataType> = HashMap::new();
        for df in &dfs {
            for (col_name, dtype) in df.get_column_names().iter().zip(df.dtypes()) {
                // Prefer non-null types
                if !column_types.contains_key(*col_name) || matches!(dtype, DataType::Null) {
                    column_types.insert(col_name.to_string(), dtype.clone());
                }
            }
        }

        // Align all dataframes to have the same columns with matching types
        let all_columns: Vec<String> = column_types.keys().cloned().collect();

        // OPTIMIZED: Use lazy concat instead of collecting each frame separately
        let aligned_lazy_dfs: Vec<LazyFrame> = dfs.into_iter().map(|df| {
            // Check which columns are missing
            let df_columns: HashSet<&str> = df.get_column_names().iter().copied().collect();
            let missing_columns: Vec<&String> = all_columns.iter()
                .filter(|col| !df_columns.contains(col.as_str()))
                .collect();

            // Start with lazy frame
            let mut lazy_df = df.lazy();

            // Add missing columns
            for col_name in missing_columns {
                let dtype = column_types.get(col_name).unwrap();
                lazy_df = lazy_df.with_column(
                    lit(NULL).cast(dtype.clone()).alias(col_name.as_str())
                );
            }

            // Select columns in consistent order (still lazy)
            lazy_df.select(&all_columns.iter().map(|s| col(s.as_str())).collect::<Vec<_>>())
        }).collect();

        // OPTIMIZED: Use Polars' built-in concat with lazy evaluation
        // This is much faster than collecting each dataframe separately
        let combined_lazy = concat(&aligned_lazy_dfs, UnionArgs::default())?;

        // Sort by datetime if column exists, then collect
        let has_datetime = all_columns.contains(&"datetime".to_string());
        let combined = if has_datetime {
            combined_lazy
                .sort("datetime", Default::default())
                .collect()?
        } else {
            combined_lazy.collect()?
        };

        Ok(combined)
    }
    
    fn detect_gaps(&self, df: &DataFrame, datetime_col: &str) -> Result<Vec<DataGap>> {
        let mut gaps = Vec::new();
        
        if !df.get_column_names().contains(&datetime_col) {
            return Ok(gaps);
        }
        
        // Get datetime column and convert i64 timestamps to dates
        let datetime_col_data = df.column(datetime_col)?;
        
        let dates = if let Ok(dt) = datetime_col_data.datetime() {
            // Already datetime type
            dt.as_datetime_iter()
                .filter_map(|dt| dt.map(|d| NaiveDate::from_ymd_opt(
                    d.year(), d.month(), d.day()
                ).unwrap()))
                .collect::<HashSet<_>>()
        } else if let Ok(timestamps) = datetime_col_data.i64() {
            // i64 timestamps in milliseconds
            timestamps.into_iter()
                .filter_map(|ts| ts.map(|t| {
                    let dt = DateTime::<Utc>::from_timestamp_millis(t).unwrap();
                    NaiveDate::from_ymd_opt(dt.year(), dt.month(), dt.day()).unwrap()
                }))
                .collect::<HashSet<_>>()
        } else {
            // Can't parse datetime column, skip gap detection
            return Ok(gaps);
        };
        
        if dates.is_empty() {
            return Ok(gaps);
        }
        
        let mut sorted_dates: Vec<_> = dates.into_iter().collect();
        sorted_dates.sort();
        
        let mut prev_date = sorted_dates[0];
        for &date in &sorted_dates[1..] {
            let days_diff = (date - prev_date).num_days();
            if days_diff > 1 {
                gaps.push(DataGap {
                    start_date: prev_date + Duration::days(1),
                    end_date: date - Duration::days(1),
                    missing_days: days_diff - 1,
                });
            }
            prev_date = date;
        }
        
        Ok(gaps)
    }
    
    fn save_gaps_report(&self, output_dir: &Path, year: i32, gaps: &[DataGap]) -> Result<()> {
        // Save Markdown report
        let report_file = output_dir.join("gaps_report.md");
        let mut content = String::new();
        
        if report_file.exists() {
            content = fs::read_to_string(&report_file)?;
        } else {
            content.push_str("# Data Gaps Report\n\n");
        }
        
        content.push_str(&format!("\n## Year {}\n", year));
        
        if gaps.is_empty() {
            content.push_str("‚úÖ No gaps detected\n");
        } else {
            content.push_str(&format!("‚ö†Ô∏è {} gaps detected:\n\n", gaps.len()));
            for gap in gaps {
                content.push_str(&format!(
                    "- {} to {} ({} days missing)\n",
                    gap.start_date, gap.end_date, gap.missing_days
                ));
            }
        }
        
        fs::write(report_file, &content)?;
        
        // Also save to CSV for easier analysis
        let csv_file = output_dir.join("gaps_report.csv");
        let mut csv_content = String::new();
        
        // Write or append CSV header
        if !csv_file.exists() {
            csv_content.push_str("dataset,year,start_date,end_date,missing_days\n");
        } else {
            csv_content = fs::read_to_string(&csv_file)?;
        }
        
        // Add gaps data
        let dataset_name = output_dir.file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("unknown");
        
        for gap in gaps {
            csv_content.push_str(&format!(
                "{},{},{},{},{}\n",
                dataset_name, year, gap.start_date, gap.end_date, gap.missing_days
            ));
        }
        
        // If no gaps, still add a record to show we checked
        if gaps.is_empty() {
            csv_content.push_str(&format!(
                "{},{},,,0\n",
                dataset_name, year
            ));
        }
        
        fs::write(csv_file, &csv_content)?;
        
        Ok(())
    }
    
    fn save_schema(&self, output_dir: &Path, _name: &str, schema_json: &str) -> Result<()> {
        let schema_file = output_dir.join("schema.json");
        fs::write(schema_file, schema_json)?;
        Ok(())
    }
    
    fn update_stats(&self, dataset: &str, year: i32, files: usize, rows: usize, gaps: Vec<DataGap>) {
        let mut stats = self.processing_stats.lock().unwrap();
        let entry = stats.entry(dataset.to_string()).or_insert_with(|| ProcessingStats {
            total_files: 0,
            processed_files: 0,
            failed_files: 0,
            total_rows: 0,
            years_covered: Vec::new(),
            gaps: Vec::new(),
            gaps_by_year: HashMap::new(),
        });

        entry.total_files += files;
        entry.processed_files += files;
        entry.total_rows += rows;
        if !entry.years_covered.contains(&year) {
            entry.years_covered.push(year);
            entry.years_covered.sort();
        }

        // Store gaps both in overall list and by year
        entry.gaps.extend(gaps.clone());
        entry.gaps_by_year.entry(year).or_insert_with(Vec::new).extend(gaps);
    }
    
    fn generate_status_report(&self) -> Result<()> {
        let stats = self.processing_stats.lock().unwrap();
        let report_file = self.output_dir.join("processing_status_report.md");

        let mut content = String::new();
        content.push_str("# ERCOT Annual Parquet Processing Status Report\n\n");
        content.push_str(&format!("Generated: {}\n\n", Utc::now().format("%Y-%m-%d %H:%M:%S UTC")));

        content.push_str("## Summary\n\n");
        content.push_str("| Dataset | Files | Rows | Years | Total Gaps |\n");
        content.push_str("|---------|-------|------|-------|------------|\n");

        for (dataset, stat) in stats.iter() {
            let gaps_str = if stat.gaps.is_empty() {
                "None".to_string()
            } else {
                format!("{} gaps", stat.gaps.len())
            };
            content.push_str(&format!(
                "| {} | {} | {} | {} | {} |\n",
                dataset,
                stat.processed_files,
                stat.total_rows,
                format!("{:?}", stat.years_covered),
                gaps_str
            ));
        }

        // Add detailed gaps by year section
        content.push_str("\n## Gap Details by Year\n\n");

        for (dataset, stat) in stats.iter() {
            if !stat.gaps_by_year.is_empty() {
                content.push_str(&format!("### {}\n\n", dataset));
                content.push_str("| Year | Gap Count | Total Missing Days | Example Gaps |\n");
                content.push_str("|------|-----------|-------------------|---------------|\n");

                let mut years: Vec<_> = stat.gaps_by_year.keys().collect();
                years.sort();

                for year in years {
                    if let Some(year_gaps) = stat.gaps_by_year.get(year) {
                        let total_missing: i64 = year_gaps.iter().map(|g| g.missing_days).sum();
                        let example_gaps = if year_gaps.len() <= 3 {
                            year_gaps.iter()
                                .map(|g| format!("{} to {} ({} days)",
                                    g.start_date.format("%Y-%m-%d"),
                                    g.end_date.format("%Y-%m-%d"),
                                    g.missing_days))
                                .collect::<Vec<_>>()
                                .join("; ")
                        } else {
                            let samples: Vec<_> = year_gaps.iter().take(2)
                                .map(|g| format!("{} to {} ({} days)",
                                    g.start_date.format("%Y-%m-%d"),
                                    g.end_date.format("%Y-%m-%d"),
                                    g.missing_days))
                                .collect();
                            format!("{}; ... and {} more", samples.join("; "), year_gaps.len() - 2)
                        };

                        content.push_str(&format!(
                            "| {} | {} | {} | {} |\n",
                            year,
                            year_gaps.len(),
                            total_missing,
                            example_gaps
                        ));
                    }
                }
                content.push_str("\n");
            }
        }

        content.push_str("## Data Quality Notes\n\n");
        content.push_str("- All price columns enforced as Float64 to prevent type mismatches\n");
        content.push_str("- Datetime columns created from delivery date and hour/interval\n");
        content.push_str("- Files processed in parallel for performance\n");
        content.push_str("- Gap detection performed on temporal data\n");
        content.push_str("- Gaps represent periods where consecutive timestamps are missing\n");

        fs::write(report_file, &content)?;

        // Also print to stdout
        println!("\n{}", "=".repeat(80));
        println!("{}", content);

        Ok(())
    }
}

#[derive(Debug)]
enum ProcessorType {
    RealTimePrice,
    RealTime5MinutePrice,
    DayAheadPrice,
    AncillaryService,
    DAMGenResource,
    DAMLoadResource,
    DAMEnergyBidAwards,
    SCEDGenResource,
    SCEDLoadResource,
    COPSnapshot,
}

fn extract_year_from_filename(path: &Path) -> Option<i32> {
    let name = path.file_name()?.to_str()?;
    
    // Look for YYYYMMDD pattern in the filename
    // Example: cdr.00012301.0000000000000000.20230104.001704.SPPHLZNP6905_20230104_0015.csv
    for part in name.split('.') {
        if part.len() >= 8 {
            // Check if starts with 20 and parse first 4 chars as year
            if part.starts_with("20") {
                if let Ok(year) = part[0..4].parse::<i32>() {
                    if (2000..=2100).contains(&year) {
                        return Some(year);
                    }
                }
            }
        }
    }
    
    None
}

fn extract_year_from_60day_filename(path: &Path) -> Option<i32> {
    let name = path.file_name()?.to_str()?;
    
    // Pattern 1: DD-MMM-YY (e.g., 07-JAN-25) for 60d_COP files
    if let Some(captures) = regex::Regex::new(r"(\d{2})-([A-Z]{3})-(\d{2})").unwrap().captures(name) {
        let year_str = captures.get(3)?.as_str();
        let year: i32 = year_str.parse().ok()?;
        // Convert 2-digit year to 4-digit
        let full_year = if year < 50 { 2000 + year } else { 1900 + year };
        return Some(full_year);
    }
    
    // Pattern 2: CompleteCOP_MMDDYYYY.csv (e.g., CompleteCOP_10132022.csv)
    if let Some(captures) = regex::Regex::new(r"CompleteCOP_(\d{2})(\d{2})(\d{4})").unwrap().captures(name) {
        let year_str = captures.get(3)?.as_str();
        let year: i32 = year_str.parse().ok()?;
        return Some(year);
    }
    
    None
}

// Helper functions for duration
fn duration_hours(hours: Expr) -> Expr {
    hours * lit(3600000)  // Convert hours to milliseconds
}